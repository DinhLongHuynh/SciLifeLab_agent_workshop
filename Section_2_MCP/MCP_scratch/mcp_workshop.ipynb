{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) Workshop ðŸ§¬\n",
    "\n",
    "Welcome! This notebook is a complete, hands-on guide to MCP, designed to take you from the fundamental concepts to advanced implementations.\n",
    "\n",
    "**What you will learn:**\n",
    "1.  **Core Concepts:** The architecture, components, and layers of MCP.\n",
    "2.  **Basic Primitives:** How to use `list`, `get`, and `call` to fetch data and run tools.\n",
    "3.  **Advanced Primitives:** How to build dynamic, two-way interactions with **Notifications**, **Elicitation**, and **Sampling**.\n",
    "4.  **Streamable HTTP:** How to use streaming for low-latency, real-time data flow, which is crucial for modern AI applications.\n",
    "5.  **LLM Integration:** How to connect MCP to a real LLM like OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 1: Core Concepts & Architecture\n",
    "\n",
    "Before we code, let's understand the foundations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Three Components\n",
    "\n",
    "MCP has three main roles that work together:\n",
    "\n",
    "1.  **ðŸ¤– MCP Host:** This is the AI application itself, like a chatbot or a language model. It's the 'brain' that needs context.\n",
    "2.  **ðŸ”Œ MCP Client:** A component that connects to servers on behalf of the Host. It's the 'adapter' that knows how to speak MCP.\n",
    "3.  **ðŸ“š MCP Server:** A program that provides the context. This could be a database, an API, or a set of tools. It's the 'library' of information and capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Two-Layer Architecture\n",
    "\n",
    "MCP is intentionally simple, consisting of two layers:\n",
    "\n",
    "- **Data Layer:** This defines the *format* of the data being exchanged (the JSON payloads). It specifies the structure for primitives like `list`, `get`, `call`, and the advanced primitives we'll see later.\n",
    "- **Transport Layer:** This defines *how* the data is moved. The standard is **HTTP(S)**. This is powerful because it leverages the existing, robust infrastructure of the web. As we'll see in the advanced section, this also allows for modern techniques like **Streamable HTTP**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 2: The Basics in Practice (Hands-on Lab 1)\n",
    "\n",
    "Let's build a simple MCP server and client to see the basic primitives in action.\n",
    "\n",
    "### Step 1: Create the Mock Data\n",
    "\n",
    "We'll start with a simple JSON file to act as our protein database. **Run the cell below** to create `protein_db.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting protein_db.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile protein_db.json\n",
    "{\n",
    "  \"P53_HUMAN\": {\"name\": \"Cellular tumor antigen p53\", \"organism\": \"Homo sapiens\", \"function\": \"Acts as a tumor suppressor.\", \"log\": \"Initializing analysis...\\nSequence loaded...\\nChecking for known mutation sites...\\nSite R248Q found...\\nGenerating report...\\nAnalysis complete.\"},\n",
    "  \"P53_MOUSE\": {\"name\": \"Cellular tumor antigen p53\", \"organism\": \"Mus musculus\", \"function\": \"Key regulator of cell cycle and apoptosis.\", \"log\": \"Initializing analysis...\\nSequence loaded...\\nNo known mutation sites found...\\nAnalysis complete.\"},\n",
    "  \"P0DTC2\": {\"name\": \"Spike glycoprotein\", \"organism\": \"SARS-CoV-2\", \"function\": \"Mediates entry into host cells.\", \"log\": \"Initializing analysis...\\nSequence loaded...\\nChecking for known mutation sites...\\nSite D614G found...\\nGenerating report...\\nAnalysis complete.\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Basic MCP Server\n",
    "\n",
    "This server will implement the `datasets/list`, `datasets/get`, and a basic `tools/call` primitive. **Run the cell below** to create `basic_server.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting basic_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile basic_server.py\n",
    "from flask import Flask, jsonify, request\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('protein_db.json') as f:\n",
    "    protein_db = json.load(f)\n",
    "\n",
    "# This primitive discovers available datasets.\n",
    "@app.route('/mcp/datasets/list', methods=['POST'])\n",
    "def list_datasets():\n",
    "    return jsonify({\"items\": [{\"id\": \"proteins\", \"name\": \"Protein Database\"}]})\n",
    "\n",
    "# This primitive retrieves the content of a specific dataset.\n",
    "@app.route('/mcp/datasets/get', methods=['POST'])\n",
    "def get_dataset():\n",
    "    data = request.json\n",
    "    if data.get('id') == 'proteins':\n",
    "        return jsonify(protein_db)\n",
    "    return jsonify({\"error\": \"Dataset not found\"}), 404\n",
    "\n",
    "# This primitive lists the available tools.\n",
    "@app.route('/mcp/tools/list', methods=['POST'])\n",
    "def list_tools():\n",
    "    return jsonify({\"items\": [\n",
    "        {\"id\": \"get_protein_function\", \"name\": \"Get Protein Function\"}\n",
    "    ]})\n",
    "\n",
    "# This primitive executes a tool.\n",
    "@app.route('/mcp/tools/call', methods=['POST'])\n",
    "def call_tool():\n",
    "    data = request.json\n",
    "    tool_id = data.get('id')\n",
    "    params = data.get('parameters', {})\n",
    "\n",
    "    if tool_id == 'get_protein_function':\n",
    "        protein_id = params.get('protein_id')\n",
    "        if protein_id in protein_db:\n",
    "            return jsonify({\"result\": protein_db[protein_id]['function']})\n",
    "        return jsonify({\"error\": \"Protein not found\"}), 404\n",
    "    \n",
    "    return jsonify({\"error\": \"Tool not found\"}), 404\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Server and Client\n",
    "\n",
    "1.  **Open a terminal**, navigate to this directory, and run,\n",
    "2. Create a virtual environment:\n",
    "   ```bash\n",
    "   python3 -m venv .venv\n",
    "   source .venv/bin/activate\n",
    "   ```\n",
    "\n",
    "3. Install dependencies:\n",
    "   ```bash\n",
    "   pip install Flask requests\n",
    "   ```\n",
    "4.  In that terminal, start the server: `python basic_server.py`\n",
    "5.  **Come back to this notebook and run the client cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Listing Datasets ---\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"id\": \"proteins\",\n",
      "      \"name\": \"Protein Database\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- 2. Getting a Dataset ---\n",
      "Successfully fetched 3 proteins.\n",
      "\n",
      "--- 3. Listing Tools ---\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"id\": \"get_protein_function\",\n",
      "      \"name\": \"Get Protein Function\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- 4. Calling a Tool ---\n",
      "{\n",
      "  \"result\": \"Acts as a tumor suppressor.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://localhost:8501/mcp\"\n",
    "\n",
    "try:\n",
    "    # 1. List Datasets\n",
    "    print(\"--- 1. Listing Datasets ---\")\n",
    "    list_resp = requests.post(f\"{BASE_URL}/datasets/list\").json()\n",
    "    print(json.dumps(list_resp, indent=2))\n",
    "\n",
    "    # 2. Get Dataset\n",
    "    print(\"\\n--- 2. Getting a Dataset ---\")\n",
    "    get_resp = requests.post(f\"{BASE_URL}/datasets/get\", json={\"id\": \"proteins\"}).json()\n",
    "    print(f\"Successfully fetched {len(get_resp)} proteins.\")\n",
    "\n",
    "    # 3. List Tools\n",
    "    print(\"\\n--- 3. Listing Tools ---\")\n",
    "    tools_resp = requests.post(f\"{BASE_URL}/tools/list\").json()\n",
    "    print(json.dumps(tools_resp, indent=2))\n",
    "\n",
    "    # 4. Call a Tool\n",
    "    print(\"\\n--- 4. Calling a Tool ---\")\n",
    "    call_resp = requests.post(f\"{BASE_URL}/tools/call\", json={\n",
    "        \"id\": \"get_protein_function\", \n",
    "        \"parameters\": {\"protein_id\": \"P53_HUMAN\"}\n",
    "    }).json()\n",
    "    print(json.dumps(call_resp, indent=2))\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"\\nERROR: Could not connect. Is basic_server.py running in a separate terminal?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 3: Advanced Concepts\n",
    "\n",
    "Basic primitives are for simple request-response. The true power of MCP comes from dynamic, two-way interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Primitives\n",
    "\n",
    "- **ðŸ”” Notifications:** A server-push mechanism. The server can proactively send information to the client without being asked. This is great for real-time updates (e.g., \"a new protein was added to the database\").\n",
    "\n",
    "- **ðŸ¤” Elicitation:** A way for the server to ask for clarification. If a client's request is ambiguous (e.g., \"find p53\"), the server can respond by asking the user to choose between \"Human p53\" and \"Mouse p53\".\n",
    "\n",
    "- **ðŸ’¡ Sampling:** A powerful feature where the server can ask the client's AI to perform a generative task. The server provides a prompt, and the client's LLM generates a completion, which is then sent back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 4: The Transport Layer in Detail - Streamable HTTP\n",
    "\n",
    "Modern AI is all about streamingâ€”getting responses token-by-token for a real-time feel. MCP fully supports this over its standard HTTP transport layer.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Streaming in MCP leverages a standard web technology called **HTTP Chunked Transfer Encoding**. Instead of sending one large response, the server sends a series of small chunks. The connection stays open until the server sends the final chunk.\n",
    "\n",
    "- **On the Server (e.g., with Flask):** You use a **generator function** with the `yield` keyword. Each time you `yield` a piece of data, it's sent to the client as a chunk. This is extremely memory efficient.\n",
    "- **On the Client (e.g., with `requests`):** You make the request with `stream=True` and then iterate over the response chunks as they arrive. \n",
    "\n",
    "This allows an MCP tool to stream back results in real time, like a log of a long-running analysis, without the user having to wait for the entire process to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 5: Advanced in Practice\n",
    "\n",
    "Now we'll build an advanced server and client that use **Notifications**, **Elicitation**, **Sampling**, and **Streaming**.\n",
    "\n",
    "### Step 1: Create the Advanced MCP Server\n",
    "\n",
    "**Stop the `basic_server.py`** in your terminal (`Ctrl+C`). **Run the cell below** to create the new `advanced_server.py`. We will run this new server in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing advanced_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile advanced_server.py\n",
    "import uuid\n",
    "import time\n",
    "from flask import Flask, jsonify, request, Response\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('protein_db.json') as f:\n",
    "    DB = {\"proteins\": json.load(f)}\n",
    "DB[\"registered_clients\"] = {}\n",
    "DB[\"pending_callbacks\"] = {}\n",
    "\n",
    "# ######################################\n",
    "# ### 1. NOTIFICATION LOGIC          ###\n",
    "# ######################################\n",
    "@app.route('/mcp/register', methods=['POST'])\n",
    "def register_client():\n",
    "    data = request.json\n",
    "    client_id, callback_url = data.get(\"client_id\"), data.get(\"callback_url\")\n",
    "    if client_id and callback_url:\n",
    "        DB[\"registered_clients\"][client_id] = callback_url\n",
    "        print(f\"Registered client '{client_id}' for Notifications.\")\n",
    "        return jsonify({\"status\": \"success\"})\n",
    "    return jsonify({\"error\": \"client_id and callback_url required\"}), 400\n",
    "\n",
    "@app.route('/mcp/events/trigger_notification', methods=['POST'])\n",
    "def trigger_notification():\n",
    "    import requests\n",
    "    for client_id, url in DB[\"registered_clients\"].items():\n",
    "        try:\n",
    "            requests.post(url, json={\"message\": \"Database was updated!\"})\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass # Ignore failed notifications for this demo\n",
    "    return jsonify({\"status\": \"ok\"})\n",
    "\n",
    "# ######################################\n",
    "# ### 3. SAMPLING LOGIC (Callback)   ###\n",
    "# ######################################\n",
    "@app.route('/mcp/callbacks/sampling_response', methods=['POST'])\n",
    "def sampling_response():\n",
    "    data = request.json\n",
    "    token, llm_result = data.get(\"callback_token\"), data.get(\"llm_result\")\n",
    "    if token in DB[\"pending_callbacks\"]:\n",
    "        DB[\"pending_callbacks\"].pop(token)\n",
    "        print(f\"--- SERVER RECEIVED SAMPLING CALLBACK ---\\n{llm_result}\")\n",
    "        return jsonify({\"status\": \"callback_received\"})\n",
    "    return jsonify({\"error\": \"Invalid token\"}), 404\n",
    "\n",
    "# --- MCP PRIMITIVES ---\n",
    "@app.route('/mcp/tools/list', methods=['POST'])\n",
    "def list_tools():\n",
    "    return jsonify({\"items\": [\n",
    "        {\"id\": \"find_protein\", \"name\": \"Find Protein (Elicitation Example)\"},\n",
    "        {\"id\": \"generate_hypothesis\", \"name\": \"Generate Hypothesis (Sampling Example)\"},\n",
    "        {\"id\": \"stream_analysis_log\", \"name\": \"Stream Analysis Log (Streaming Example)\"}\n",
    "    ]})\n",
    "\n",
    "@app.route('/mcp/tools/call', methods=['POST'])\n",
    "def call_tool():\n",
    "    data = request.json\n",
    "    tool_id, params = data.get('id'), data.get('parameters', {})\n",
    "\n",
    "    # ######################################\n",
    "    # ### 2. ELICITATION LOGIC           ###\n",
    "    # ######################################\n",
    "    if tool_id == 'find_protein':\n",
    "        protein_name = params.get('protein_name', '').lower()\n",
    "        if protein_name == 'p53':\n",
    "            return jsonify({\n",
    "                \"result_type\": \"elicitation\",\n",
    "                \"message\": \"Multiple proteins match 'p53'. Please specify:\",\n",
    "                \"choices\": [\n",
    "                    {\"label\": \"Human p53\", \"value\": \"P53_HUMAN\"},\n",
    "                    {\"label\": \"Mouse p53\", \"value\": \"P53_MOUSE\"}\n",
    "                ]\n",
    "            })\n",
    "        return jsonify({\"error\": \"Protein not found\"})\n",
    "\n",
    "    # ######################################\n",
    "    # ### 3. SAMPLING LOGIC (Request)    ###\n",
    "    # ######################################\n",
    "    if tool_id == 'generate_hypothesis':\n",
    "        protein_id = params.get('protein_id')\n",
    "        if protein_id in DB[\"proteins\"]:\n",
    "            info = DB[\"proteins\"][protein_id]\n",
    "            token = str(uuid.uuid4())\n",
    "            DB[\"pending_callbacks\"][token] = {\"protein_id\": protein_id}\n",
    "            prompt = f\"The protein {info['name']} is known to {info['function']}. Generate a novel research hypothesis.\"\n",
    "            return jsonify({\"result_type\": \"sampling\", \"prompt\": prompt, \"callback_token\": token})\n",
    "        return jsonify({\"error\": \"Protein not found\"})\n",
    "        \n",
    "    return jsonify({\"error\": \"Tool not found\"})\n",
    "\n",
    "# ######################################\n",
    "# ### 4. STREAMING LOGIC             ###\n",
    "# ######################################\n",
    "@app.route('/mcp/tools/stream', methods=['POST'])\n",
    "def stream_tool():\n",
    "    data = request.json\n",
    "    tool_id, params = data.get('id'), data.get('parameters', {})\n",
    "\n",
    "    if tool_id == 'stream_analysis_log':\n",
    "        protein_id = params.get('protein_id')\n",
    "        if protein_id in DB[\"proteins\"]:\n",
    "            log_data = DB[\"proteins\"][protein_id].get(\"log\", \"No log available.\")\n",
    "            \n",
    "            def generate_chunks():\n",
    "                # This generator function yields chunks of the response.\n",
    "                for line in log_data.split('\\n'):\n",
    "                    chunk = json.dumps({\"log_entry\": line}) + \"\\n\" # Send each line as a JSON object\n",
    "                    yield chunk\n",
    "                    time.sleep(0.5) # Simulate a delay for demonstration\n",
    "            \n",
    "            # Return a streaming response.\n",
    "            return Response(generate_chunks(), mimetype='application/x-ndjson')\n",
    "        return Response(json.dumps({\"error\": \"Protein not found\"}), status=404)\n",
    "        \n",
    "    return Response(json.dumps({\"error\": \"Streamable tool not found\"}), status=404)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=8501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the Advanced Server and Client\n",
    "\n",
    "1.  **Open a terminal** and install dependencies: `pip install Flask requests openai`\n",
    "2.  **Set your OpenAI API Key** in that terminal. This is required for the Sampling feature.\n",
    "    - *macOS/Linux:* `export OPENAI_API_KEY=\"sk-...\"`\n",
    "    - *Windows:* `set OPENAI_API_KEY=\"sk-...\"`\n",
    "3.  **Start the advanced server** in the terminal: `python advanced_server.py`\n",
    "4.  **Come back here and run the advanced client cell below.** This single cell will run a client that demonstrates all advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal client server starting on port 8505 to listen for notifications...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. DEMONSTRATING ELICITATION ---\n",
      "Server asked for clarification: Multiple proteins match 'p53'. Please specify:\n",
      "Choices: [{\"label\": \"Human p53\", \"value\": \"P53_HUMAN\"}, {\"label\": \"Mouse p53\", \"value\": \"P53_MOUSE\"}]\n",
      "\n",
      "--- 3. DEMONSTRATING SAMPLING ---\n",
      "Server sent a prompt for the LLM:\n",
      "'The protein Spike glycoprotein is known to Mediates entry into host cells.. Generate a novel research hypothesis.'\n",
      "LLM responded. Sending result back to server.\n",
      "\n",
      "--- 4. DEMONSTRATING STREAMING ---\n",
      "Client is receiving a real-time stream from the server:\n",
      "  -> Initializing analysis...\n",
      "  -> Sequence loaded...\n",
      "  -> Checking for known mutation sites...\n",
      "  -> Site R248Q found...\n",
      "  -> Generating report...\n",
      "  -> Analysis complete.\n",
      "\n",
      "Client workflow finished. Listening for notifications...\n",
      "(To test notifications, stop this cell and run the trigger cell below)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- ðŸ”” NOTIFICATION RECEIVED! ðŸ”” ---\n",
      "{\n",
      "  \"message\": \"Database was updated!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This cell contains a complete, advanced MCP client.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from threading import Thread\n",
    "from flask import Flask, jsonify, request # Import jsonify\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "# --- LLM & Client CONFIG ---\n",
    "try:\n",
    "    llm_client = OpenAI()\n",
    "except Exception as e:\n",
    "    print(\"WARNING: OpenAI client failed to init. Is OPENAI_API_KEY set? Sampling will be skipped.\")\n",
    "    llm_client = None\n",
    "\n",
    "# Corrected port to match what the client server will use.\n",
    "MCP_SERVER_URL = \"http://localhost:8501\"\n",
    "CLIENT_PORT = 8505 # Define the port as a variable for consistency\n",
    "CLIENT_CALLBACK_URL = \"http://localhost:\"+str(CLIENT_PORT)+\"/mcp_callback\" # Our client's own address\n",
    "\n",
    "\n",
    "# Disable Flask's default logging to keep the output clean\n",
    "log = logging.getLogger('werkzeug')\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "# ######################################\n",
    "# ### 1. NOTIFICATION HANDLING       ###\n",
    "# ######################################\n",
    "client_app = Flask(__name__)\n",
    "@client_app.route('/mcp_callback', methods=['POST'])\n",
    "def mcp_callback():\n",
    "    \"\"\"This server endpoint listens for incoming Notifications.\"\"\"\n",
    "    print(\"\\n\\n--- ðŸ”” NOTIFICATION RECEIVED! ðŸ”” ---\")\n",
    "    print(json.dumps(request.json, indent=2))\n",
    "    return jsonify({\"status\": \"ok\"})\n",
    "\n",
    "def run_client_server():\n",
    "    \"\"\"This server runs in a background thread to listen for notifications.\"\"\"\n",
    "    print(f\"Internal client server starting on port {CLIENT_PORT} to listen for notifications...\")\n",
    "    try:\n",
    "        client_app.run(port=CLIENT_PORT)\n",
    "    except OSError as e:\n",
    "        # Catch the \"address in use\" error specifically\n",
    "        print(f\"\\nERROR: Port {CLIENT_PORT} is already in use. Please stop the other program or change the CLIENT_PORT variable.\")\n",
    "        # Exiting the thread will allow the main script to finish.\n",
    "        return \n",
    "\n",
    "# --- Main Client Logic ---\n",
    "def run_advanced_client_workflow():\n",
    "    # Register for notifications\n",
    "    requests.post(f\"{MCP_SERVER_URL}/mcp/register\", json={\n",
    "        \"client_id\": \"advanced_notebook_client\", \n",
    "        \"callback_url\": CLIENT_CALLBACK_URL\n",
    "    })\n",
    "\n",
    "    # ######################################\n",
    "    # ### 2. ELICITATION DEMO            ###\n",
    "    # ######################################\n",
    "    print(\"\\n--- 2. DEMONSTRATING ELICITATION ---\")\n",
    "    elicitation_resp = requests.post(f\"{MCP_SERVER_URL}/mcp/tools/call\", json={\n",
    "        \"id\": \"find_protein\", \n",
    "        \"parameters\": {\"protein_name\": \"p53\"}\n",
    "    }).json()\n",
    "    if elicitation_resp.get(\"result_type\") == \"elicitation\":\n",
    "        print(f\"Server asked for clarification: {elicitation_resp['message']}\")\n",
    "        print(f\"Choices: {json.dumps(elicitation_resp['choices'])}\")\n",
    "\n",
    "    # ######################################\n",
    "    # ### 3. SAMPLING DEMO               ###\n",
    "    # ######################################\n",
    "    print(\"\\n--- 3. DEMONSTRATING SAMPLING ---\")\n",
    "    sampling_resp = requests.post(f\"{MCP_SERVER_URL}/mcp/tools/call\", json={\n",
    "        \"id\": \"generate_hypothesis\", \n",
    "        \"parameters\": {\"protein_id\": \"P0DTC2\"}\n",
    "    }).json()\n",
    "    if sampling_resp.get(\"result_type\") == \"sampling\" and llm_client:\n",
    "        prompt = sampling_resp['prompt']\n",
    "        print(f\"Server sent a prompt for the LLM:\\n'{prompt}'\")\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        llm_result = completion.choices[0].message.content\n",
    "        print(f\"LLM responded. Sending result back to server.\")\n",
    "        requests.post(f\"{MCP_SERVER_URL}/mcp/callbacks/sampling_response\", json={\n",
    "            \"callback_token\": sampling_resp['callback_token'], \n",
    "            \"llm_result\": llm_result\n",
    "        })\n",
    "    elif not llm_client:\n",
    "        print(\"Skipping Sampling demo because OpenAI client is not configured.\")\n",
    "\n",
    "    # ######################################\n",
    "    # ### 4. STREAMING DEMO              ###\n",
    "    # ######################################\n",
    "    print(\"\\n--- 4. DEMONSTRATING STREAMING ---\")\n",
    "    with requests.post(f\"{MCP_SERVER_URL}/mcp/tools/stream\", json={\n",
    "        \"id\": \"stream_analysis_log\", \n",
    "        \"parameters\": {\"protein_id\": \"P53_HUMAN\"}\n",
    "    }, stream=True) as r:\n",
    "        print(\"Client is receiving a real-time stream from the server:\")\n",
    "        for line in r.iter_lines():\n",
    "            if line:\n",
    "                log_entry = json.loads(line)\n",
    "                print(f\"  -> {log_entry['log_entry']}\")\n",
    "    \n",
    "    print(\"\\nClient workflow finished. Listening for notifications...\")\n",
    "    print(\"(To test notifications, stop this cell and run the trigger cell below)\")\n",
    "\n",
    "# --- Run Everything ---\n",
    "try:\n",
    "    # Start the client's notification listener in the background\n",
    "    client_server_thread = Thread(target=run_client_server, daemon=True)\n",
    "    client_server_thread.start()\n",
    "    \n",
    "    # Run the main workflow\n",
    "    run_advanced_client_workflow()\n",
    "    \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"\\nERROR: Could not connect. Is advanced_server.py running in a separate terminal?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Trigger a Notification (Optional)\n",
    "\n",
    "After the cell above finishes, your client's web server is still running in the background listening for notifications. **Run the cell below** to send a request to your MCP server, telling it to push a notification to your client. You will see the notification message appear in the output of the cell *above*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telling the server to send a notification...\n",
      "Trigger request sent. Check the output of the cell above for the notification!\n"
     ]
    }
   ],
   "source": [
    "# This cell triggers the notification.\n",
    "import requests\n",
    "try:\n",
    "    print(\"Telling the server to send a notification...\")\n",
    "    requests.post(\"http://localhost:8501/mcp/events/trigger_notification\")\n",
    "    print(\"Trigger request sent. Check the output of the cell above for the notification!\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"\\nERROR: Could not connect. Is advanced_server.py still running?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 6: Conclusion\n",
    "\n",
    "Congratulations! You have successfully built and tested a complete MCP system.\n",
    "\n",
    "You have learned how to:\n",
    "- Structure an AI application with the **Host, Client, and Server** components.\n",
    "- Use basic primitives like **`list`, `get`, and `call`** for simple request-response.\n",
    "- Create sophisticated, two-way interactions using **Notifications, Elicitation, and Sampling**.\n",
    "- Implement low-latency, real-time data flows with **Streamable HTTP**.\n",
    "\n",
    "MCP provides a simple yet powerful standard for making AI applications more capable, interactive, and connected to the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
