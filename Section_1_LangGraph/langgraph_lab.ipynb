{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41992144",
   "metadata": {},
   "source": [
    "<img src=\"images/scilife_logo.png\" width=\"400\">\n",
    "<img src=\"images/essence_logo.png\" width=\"300\">\n",
    "\n",
    "# SciLifeLab Workshop - Hands-on Section: LangGraph \"Hello World\"\n",
    "\n",
    "\n",
    "\n",
    "Welcome to this handsâ€‘on lab session on building AI Agents with **LangGraph**! \n",
    "\n",
    "LangGraph is a lowâ€‘level orchestration framework for constructing stateful AI workflows using graphs. LangGraph provides several key benefits for agentic AI applications, including durable execution, support for humanâ€‘inâ€‘theâ€‘loop workflows, comprehensive memory (both shortâ€‘ and longâ€‘term), builtâ€‘in debugging, and productionâ€‘ready deployment features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a60e0",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this workshop, you will:\n",
    "- Understand core concepts of LangGraph (tools, nodes, edges, state, and memory).\n",
    "- Create and integrate your own tools for an AI agent.\n",
    "- Build a ReActâ€‘style agent using an LLM and custom tools.\n",
    "- Implement agent memory to maintain conversational context.\n",
    "- Compare custom agents with prebuilt LangGraph agents.\n",
    "- Explore extension tasks such as custom graph, structured output and prompts template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841aa80a",
   "metadata": {},
   "source": [
    "## Workshop Outline \n",
    "- **PartÂ 1**: Setup & imports\n",
    "- **PartÂ 2**: Understanding and creating tools\n",
    "- **PartÂ 3**: Defining the state\n",
    "- **PartÂ 4**: Building the agent graph\n",
    "- **PartÂ 5**: Testing the agent\n",
    "- **PartÂ 6**: Adding shortâ€‘term memory\n",
    "- **PartÂ 7**: Exploring prebuilt agents\n",
    "- **PartÂ 8 (optional)**: Extension exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f95d8",
   "metadata": {},
   "source": [
    "## Instructions for Participants\n",
    "\n",
    "**Throughout this lab, look for `TODO` comments and `...` placeholders in the code cells. These indicate where you need to add your implementation.** \n",
    "\n",
    "Your task is to:\n",
    "1. Replace `...` placeholders with appropriate code\n",
    "2. Follow the instructions in `TODO` comments \n",
    "3. Refer to the exercise descriptions and API references provided\n",
    "\n",
    "**Tip:** Each exercise builds on the previous one, so complete them in order!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ba369",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1 â€“ Setup\n",
    "\n",
    "In this first step, we'll import the necessary dependencies and load any environment variables. Make sure your API keys (e.g. OpenAI) are stored in a `.env` file in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a6198",
   "metadata": {},
   "source": [
    "### ExerciseÂ 1.1 â€“ Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any packages you need here\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import pubchempy as pcp\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from langchain.tools import tool\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8cdae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PartÂ 2Â â€“ Understanding and Creating Tools\n",
    "\n",
    "In LangGraph, **tools** are Python functions that extend your agent's capabilities beyond text generation. They can call external APIs or perform computations, and are annotated with the `@tool` decorator from LangChain. \n",
    "\n",
    "When designing tools for agents, there are two important considerations: \n",
    "- **Input - Output**: It is similar to when designing a Python function, but the input is now handled by generated content from LLM. Output should be parsed to feed meaningful context to the agent\n",
    "\n",
    "- **Description**: The `@tool` decorator requires docstrings from a Python function and will use it as the tool's description. These descriptions will feed the LLM system prompts, making it aware of these existing tools and their schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fac64e",
   "metadata": {},
   "source": [
    "### About the task\n",
    "\n",
    "In this exercise, we will develop three simple tools, which could be useful for the drug discovery.\n",
    "\n",
    "**1. SMILES Resolver:** a tool to get SMILES string for compound from any identifers\n",
    "\n",
    "**2. Get Properties:** a tool to get basic physiochemical properties of the molecule\n",
    "\n",
    "**3. Lit Search:** a tool to perform semantic search among all publications available on PubMed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57d201",
   "metadata": {},
   "source": [
    "#### **Tool 1: SMILES resolver**\n",
    "This tool takes any identifiers of compounds, i.e. compound name, chembl id, pubchem cid, cas number, etc. It returns a dictionary containing original identifiers and queried SMILES string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa198dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to external API\n",
    "CACTUS_BASE = \"https://cactus.nci.nih.gov/chemical/structure\"\n",
    "molecule = new_client.molecule\n",
    "\n",
    "# Helper functions\n",
    "def _is_chembl_id(value: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"CHEMBL\\d+\", value.upper()))\n",
    "\n",
    "def _looks_like_inchikey(value: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"[A-Z]{14}-[A-Z]{10}-[A-Z]\", value))\n",
    "\n",
    "\n",
    "# Main tool\n",
    "@tool\n",
    "def resolve_smiles(\n",
    "    identifier: str,\n",
    "    pause_s: float = 0.0,\n",
    "    timeout_s: float = 15.0,\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Resolve any identifier types to canonical SMILES.\n",
    "    \n",
    "    Args:\n",
    "        identifier: any identifiers of compounds, i.e. name, CID, CHEMBL, CAS, etc.\n",
    "    \n",
    "    Returns: \n",
    "        dict: original identifiers and corresponding SMILES string\n",
    "    \"\"\"\n",
    "    ident = identifier.strip()\n",
    "    if not ident:\n",
    "        return None\n",
    "    \n",
    "    ident_lower = ident.lower()\n",
    "    ident_upper = ident.upper()\n",
    "    \n",
    "    # Try ChEMBL\n",
    "    if _is_chembl_id(ident_upper):\n",
    "        try:\n",
    "            mol = molecule.get(ident_upper)\n",
    "            smiles = mol.get(\"molecule_structures\", {}).get(\"canonical_smiles\")\n",
    "            if smiles:\n",
    "                if pause_s:\n",
    "                    time.sleep(pause_s)\n",
    "                return {'identifier': identifier, 'SMILES': smiles}\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    # Try CACTUS\n",
    "    try:\n",
    "        url = f\"{CACTUS_BASE}/{quote(ident)}/smiles\"\n",
    "        response = requests.get(url, timeout=timeout_s, headers={\"User-Agent\": \"smiles-resolver/1.0\"})\n",
    "        if response.ok:\n",
    "            text = response.text.strip()\n",
    "            # Explicitly check for non-empty and valid content\n",
    "            if (text  and len(text) > 0 ):\n",
    "                if pause_s:\n",
    "                    time.sleep(pause_s)\n",
    "                return {'identifier': identifier, 'SMILES': text}\n",
    "            # If empty or invalid, fall through to PubChem\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    \n",
    "    # Try PubChem\n",
    "    try:\n",
    "        if ident.isdigit():\n",
    "            compound = pcp.Compound.from_cid(int(ident))\n",
    "            smiles = getattr(compound, \"canonical_smiles\", None)\n",
    "            if smiles:\n",
    "                if pause_s:\n",
    "                    time.sleep(pause_s)\n",
    "                return {'identifier': identifier, 'SMILES': smiles}\n",
    "        \n",
    "        if _looks_like_inchikey(ident_upper):\n",
    "            compounds = pcp.get_compounds(ident_upper, namespace=\"inchikey\")\n",
    "        else:\n",
    "            compounds = pcp.get_compounds(ident_lower, namespace=\"name\")\n",
    "        \n",
    "        if compounds:\n",
    "            smiles = getattr(compounds[0], \"canonical_smiles\", None)\n",
    "            if smiles:\n",
    "                if pause_s:\n",
    "                    time.sleep(pause_s)\n",
    "                return {'identifier': identifier, 'SMILES': smiles}\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89071460",
   "metadata": {},
   "source": [
    "#### **Tool 2: Get Properties**\n",
    "This tool takes SMILES string as input and output a dictionary containing SMILES string and its physiochemical properties, i.e. MW, logP, HBA, HBA, TPSA, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool \n",
    "def get_properties(smiles: str):\n",
    "    # TODO: Write description for this function\n",
    "    \"\"\"\n",
    "    What does it do?\n",
    "\n",
    "    What are the args?\n",
    "\n",
    "    What are the output (return)?\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    if mol is None:\n",
    "        return {\"error\": \"Invalid SMILES string\"}\n",
    "    \n",
    "    # TODO: Gat all desired properties\n",
    "    # You can get more properties if you want\n",
    "    properties = {\n",
    "        \"Molecular_Weight\": Descriptors..., # get Molecular Weight\n",
    "        \"LogP\": Descriptors..., # get Molecular logP\n",
    "        \"HBD\": Descriptors...,  # get number of hydrogen bond donors\n",
    "        \"HBA\": Descriptors...,  # get number of hydrogen bond acceptors\n",
    "        \"TPSA\": Descriptors...,  # get TPSA\n",
    "        \"Rotatable_Bonds\": Descriptors..., # get number of rotatable bonds\n",
    "        \"Aromatic_Rings\": Descriptors..., # get number of aromatic rings\n",
    "        \"Heavy_Atoms\": Descriptors..., # get number of heavy atoms\n",
    "        \"Formal_Charge\": Chem.GetFormalCharge(mol),\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'SMILES': smiles,\n",
    "        'Properties': properties\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda5cfb0",
   "metadata": {},
   "source": [
    "#### **Tool 3: Lit Search**\n",
    "This tool processes a natural language query and uses semantic search to retrieve the most relevant passages from all PubMed publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from utils.litsense.litsense import LitSense_API\n",
    "\n",
    "@tool\n",
    "def lit_search(query: str, limit: int = 20, rerank: bool =True) -> str:\n",
    "    \"\"\"Retrieve information from PubMed using a semantic search via the LitSense API.\\n\\n\n",
    "    \n",
    "    Args:\n",
    "        query: The research question or topic to search for in PubMed literature.\n",
    "        limit: Maximum number of results to return (default is 5).\n",
    "        rerank: Whether to rerank the relevance of retrieved paragraphs.\n",
    "    \n",
    "    Returns:\n",
    "        str: all the paragraphs + corresponding PMID number.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        engine = LitSense_API()\n",
    "        results = engine.retrieve(query, limit=limit, rerank=rerank)\n",
    "        if not results:\n",
    "            return f\"No relevant literature found for '{query}'. Please try a different or broader search query.\"\n",
    "        result_str = \"\"\n",
    "        for i, result in enumerate(results):\n",
    "            result_str += (\n",
    "                f\"\\n--- Passage #{i+1} ---\\n\"\n",
    "                f\"PMID: {result.pmid}\\n\"\n",
    "                f\"Content: {result.text}\\n\"\n",
    "            )\n",
    "        return result_str\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving literature for '{query}': {str(e)}. Please try a different search query.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc026a68",
   "metadata": {},
   "source": [
    "### ExerciseÂ 2.4 â€“ Create a Tools List\n",
    "\n",
    "Collect all of your tool functions into a single list called `TOOLS`. This list will be passed to the LLM so that it is aware of what tools are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72970c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put all your tools function into TOOLS list\n",
    "TOOLS = [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99f429b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PartÂ 3Â â€“ Understanding LangGraph State\n",
    "\n",
    "For agents, the state typically contains a list of messages which grows over the conversation. \n",
    "\n",
    "When creating a state schema, you use `TypedDict` to describe the keys and `Annotated` with updating functions to specify how values should be updated. \n",
    "\n",
    "The `add_messages` function appends new messages rather than overwriting them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf42e1",
   "metadata": {},
   "source": [
    "### ExerciseÂ 3.1 â€“ Define Chat State\n",
    "\n",
    "Define a `ChatState` class (subclassing `TypedDict`) with a single key `messages`. Use the `add_messages` updating function so that new messages are appended to the state.\n",
    "\n",
    "<img src=\"images/ChatState.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f573a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, List, Dict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    \"\"\"The state schema for our LangGraph. It contains only a list of messages.\n",
    "\n",
    "    Messages are appended via the `add_messages` to preserve the full conversation history.\"\"\"\n",
    "    messages: Annotated[List[Dict], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f000fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PartÂ 4Â â€“ Building the Agent Graph\n",
    "\n",
    "This is the graph we are going to create: \n",
    "\n",
    "<img src=\"images/react_agent.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2f68f",
   "metadata": {},
   "source": [
    "### ExerciseÂ 4.1 â€“ Initialize Components\n",
    "\n",
    "1. Initialise a `StateGraph` instance using your `ChatState`.\n",
    "2. Initialise a chat model with `ChatOpenAI(model='gpt-4o', temperature=0)`.\n",
    "3. Bind your tools to the LLM using `bind_tools()`.\n",
    "4. Create a `ToolNode` from your tools list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdadcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create graph builder using ChatState\n",
    "graph_builder = StateGraph(ChatState)\n",
    "\n",
    "# TODO: Initialise the chat model \n",
    "llm = ChatOpenAI(model='...', temperature=0)\n",
    "\n",
    "# TODO: Bind the tools to the LLM so that it knows their schemas and how to construct tool calls in JSON\n",
    "llm_with_tools = llm.bind_tools(..., parallel_tool_calls=False)\n",
    "\n",
    "# TODO: Create a ToolNode using our tools list\n",
    "tool_node = ToolNode(tools=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fc510",
   "metadata": {},
   "source": [
    "### ExerciseÂ 4.2 â€“ Define the Chatbot Node\n",
    "\n",
    "Define a chatbot function that takes the `state` as input and returns a dictionary with a single key `messages`.\n",
    "\n",
    "When invoke the LLM, we provide it with all the `messages` in `state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def chatbot(state: ChatState) -> Dict[str, List]:\n",
    "    \"\"\"The main chatbot node. It invokes the LLM with the current messages.\"\"\"\n",
    "    return {'messages': [llm_with_tools.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265e466",
   "metadata": {},
   "source": [
    "### ExerciseÂ 4.3 â€“ Define the Routing Function\n",
    "\n",
    "Create a function `route_tools` that decides whether to call tools or terminate. \n",
    "\n",
    "- Step 1: Extract the last message from `state`.\n",
    "- Step 2: If the last message contains tool calls (`tool_calls` attribute), return `'tools'`; otherwise return `'END'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the output of route function\n",
    "def route_tools(state: ChatState) -> str:\n",
    "    messages = state.get('messages', [])\n",
    "    ai_message = messages[-1] if messages else None\n",
    "    if ai_message and getattr(ai_message, 'tool_calls', []):\n",
    "        return ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b54960",
   "metadata": {},
   "source": [
    "### ExerciseÂ 4.4 â€“ Build the Complete Graph\n",
    "\n",
    "Assemble the agent graph by adding nodes and edges, then compile the graph into a runnable agent. Use `add_node()` for your chatbot and tool nodes, `add_conditional_edges()` for routing logic, and `add_edge()` to connect nodes.\n",
    "\n",
    "\n",
    "<img src=\"images/graph_create_1.png\" width=\"800\">\n",
    "\n",
    "<img src=\"images/graph_create_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(ChatState)\n",
    "\n",
    "# TODO: Add the chatbot node to the graph\n",
    "graph_builder.add_node('...', ...)\n",
    "\n",
    "# TODO: Add the tool node to the graph\n",
    "graph_builder.add_node('...', ...)\n",
    "\n",
    "# TODO: Add conditional edges from chatbot using the routing function\n",
    "graph_builder.add_conditional_edges('...', route_tools, {'...': '...', '...': ...})\n",
    "\n",
    "# TODO: Add edge from tools back to chatbot\n",
    "graph_builder.add_edge('...', '...')\n",
    "\n",
    "# TODO: Add edge from START to chatbot\n",
    "graph_builder.add_edge(..., '...')\n",
    "\n",
    "# Compile the graph into an agent\n",
    "agent = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1abcd",
   "metadata": {},
   "source": [
    "### ExerciseÂ 4.5 â€“ Visualize Your Agent\n",
    "\n",
    "Use the graph's `draw_mermaid_png()` method to visualize the structure of your agent. This step is optional but helps you understand how nodes and edges connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942675e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print('Graph visualization not available in this environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf362da6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PartÂ 5Â â€“ Testing Your Agent\n",
    "\n",
    "Now that your agent graph is built, it's time to interact with it. Create a simple chat loop that greets the user, processes input until they type 'quit', and streams responses using `agent.stream()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f916",
   "metadata": {},
   "source": [
    "### ExerciseÂ 5.1 â€“ Create a Basic Chat Loop\n",
    "\n",
    "Write an interactive loop that:\n",
    "1. Greets the user and explains the agent's capabilities.\n",
    "2. Reads user input in a loop and exits on `'quit'`, `'exit'` or `'q'`.\n",
    "3. Creates the initial `state` with the user's message and streams the agent's responses via `agent.stream()`.\n",
    "4. Uses the `pretty_print()` method on messages to display nicely formatted output.\n",
    "\n",
    "\n",
    "Try out this chat loop with these three prompts: \n",
    "\n",
    "- **Prompt 1**:Â â€ You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data. Here is the compound ID: Erlotinib. Is it orally drug-like?â€\n",
    "\n",
    "\n",
    "- **Prompt 2**: \" You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data. Which is more lipophilic: Imatinib or Dasatinib?â€\n",
    "\n",
    "\n",
    "- **Prompt 3**:Â  \" You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data. Check betulinic acid. Are there any structural red flags for oral bioavailability?â€\n",
    "\n",
    "- **Prompt 4**:Â  \" You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data. What are known EGFR inhibitors in the literature, and are they drug-like?â€\n",
    "\n",
    "- **Prompt 5**:Â  \" You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data. What are the common physicochemical characteristics of successful JAK inhibitors?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Welcome to the LangGraph ReAct demo with memory! Ask me a question.')\n",
    "print('I can perform simple math, look up drug information, and search PubMed via LitSense.')\n",
    "print('====================================================================')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get input prompt from user\n",
    "        user_input = input('User: ')\n",
    "    except EOFError:\n",
    "        break\n",
    "    \n",
    "    # If input contains one of three keywords: quit, exit, and q; exit the streaming process.\n",
    "    if not user_input or user_input.lower() in {'quit', 'exit', 'q'}:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    # Initialize graph state with user's input\n",
    "    state = {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': user_input}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Print the user's message first\n",
    "    print(f\"\"\"================================== User's Message ==================================\n",
    "{user_input}\n",
    "    \"\"\")\n",
    "    # Stream the answer from agent\n",
    "    for event in agent.stream(state):\n",
    "        for value in event.values():\n",
    "            # The last message in the state is the AI response\n",
    "            msg = value[\"messages\"][-1]\n",
    "            # Use the built-in pretty_print method for better formatting\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dcfa59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Adding Shortâ€‘term Memory\n",
    "\n",
    "To demonstrate the lack of memory in the agent, please **go back to Part 5**, then perform these two prompts: \n",
    "\n",
    "- **Prompt 1:** Here is the compound ID: Erlotinib. Is it orally drug-like?\n",
    "\n",
    "- **Prompt 2:** Which is more lipophilic: Imatinib or the previous drug?\n",
    "\n",
    "You will see when invoking **prompt 2**, the agent get confuse since it doesn't know the previous drug is Erlotinib. This is because the GraphState is only maintained from the START node to the END node. After termination, all contexts from the previous run are lost. Therefore, we need a component called Agent Memory to maintain context across runs.\n",
    "\n",
    "Memory allows your agent to remember previous parts of the conversation. LangGraph uses \"checkpointers\" to maintain state across interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80071d66",
   "metadata": {},
   "source": [
    "### ExerciseÂ 6.1 â€“ Add Memory to Your Agent\n",
    "\n",
    "Rebuild your agent with memory by creating a new `StateGraph` that uses the same `ChatState`. \n",
    "\n",
    "Initialize `checkpointer` with `InMemorySaver` to persist state across turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922def4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[List[Dict], add_messages]\n",
    "\n",
    "graph_builder = StateGraph(ChatState)\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "llm_with_tools = llm.bind_tools(TOOLS, parallel_tool_calls=False)\n",
    "\n",
    "def chatbot(state: ChatState) -> Dict[str, List]:\n",
    "    return {'messages': [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "tool_node = ToolNode(tools=TOOLS)\n",
    "\n",
    "graph_builder.add_node('chatbot', chatbot)\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "graph_builder.add_conditional_edges('chatbot', route_tools, {'tools': 'tools', 'END': END})\n",
    "graph_builder.add_edge('tools', 'chatbot')\n",
    "graph_builder.add_edge(START, 'chatbot')\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "agent = graph_builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc59c2e",
   "metadata": {},
   "source": [
    "### ExerciseÂ 6.2 â€“ Create a Memoryâ€‘Enabled Chat Loop\n",
    "\n",
    "Write a chat loop similar to PartÂ 5, but supply a `config` dictionary to `agent.stream()`. In the config, there are two important parameters, including `thread_id` and `recursion_limit`.\n",
    "\n",
    "<img src=\"images/short_term_mem.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create config for memory\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': '...',\n",
    "    },\n",
    "    'recursion_limit': ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af68cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Welcome to the LangGraph ReAct demo with memory! Ask me a question.')\n",
    "print('I can perform simple math, look up drug information, and search PubMed via LitSense.')\n",
    "print('====================================================================')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get input prompt from user\n",
    "        user_input = input('User: ')\n",
    "    except EOFError:\n",
    "        break\n",
    "    \n",
    "    # If input contains one of three keywords: quit, exit, and q; exit the streaming process.\n",
    "    if not user_input or user_input.lower() in {'quit', 'exit', 'q'}:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    # Initialize graph state with user's input\n",
    "    state = {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': user_input}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    # Print the user's message first\n",
    "    print(f\"\"\"================================== User's Message ==================================\n",
    "{user_input}\n",
    "    \"\"\")\n",
    "    \n",
    "    # TODO: Supply config for the agent stream process\n",
    "    for event in agent.stream(state, config=...):\n",
    "        for value in event.values():\n",
    "            # The last message in the state is the AI response\n",
    "            msg = value[\"messages\"][-1]\n",
    "            # Use the built-in pretty_print method for better formatting\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037557d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PartÂ 7Â â€“ Prebuilt Agents\n",
    "\n",
    "LangGraph provides prebuilt agents that implement common architectures such as the ReAct pattern. These agents are quick to set up and let you focus on your tools and prompts rather than on graph wiring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a0d1b",
   "metadata": {},
   "source": [
    "### ExerciseÂ 7.1 â€“ Create a Prebuilt Agent\n",
    "\n",
    "Use `create_agent` to instantiate a prebuilt ReAct agent. Provide your LLM, tools list, and a custom system prompt that instructs the agent how to behave.\n",
    "\n",
    "#### References: [LangGraph Prebuilt Tutorial](https://langchain-ai.github.io/langgraph/agents/agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a284229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for memory\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'example_conversation',\n",
    "    },\n",
    "    'recursion_limit': 25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model = llm,\n",
    "    tools = TOOLS,\n",
    "    system_prompt=\"You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data.\",\n",
    "    checkpointer = checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c88a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Welcome to the LangGraph ReAct demo with memory! Ask me a question.')\n",
    "print('I can perform simple math, look up drug information, and search PubMed via LitSense.')\n",
    "print('====================================================================')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get input prompt from user\n",
    "        user_input = input('User: ')\n",
    "    except EOFError:\n",
    "        break\n",
    "    \n",
    "    # If input contains one of three keywords: quit, exit, and q; exit the streaming process.\n",
    "    if not user_input or user_input.lower() in {'quit', 'exit', 'q'}:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    # Initialize graph state with user's input\n",
    "    state = {\n",
    "        'messages': [\n",
    "            {'role': 'user', 'content': user_input}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Print the user's message first\n",
    "    print(f\"\"\"================================== User's Message ==================================\n",
    "{user_input}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Supply config for the agent stream process\n",
    "    for event in agent.stream(state, config=config):\n",
    "        for value in event.values():\n",
    "            # The last message in the state is the AI response\n",
    "            msg = value[\"messages\"][-1]\n",
    "            # Use the built-in pretty_print method for better formatting\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb6f7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PartÂ 8Â â€“ Extension Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d23b97",
   "metadata": {},
   "source": [
    "### Exercise 8.1 â€“ Create a system prompt using a prompt template\n",
    "\n",
    "In LangGraph, you can provide a **system** or **instruction** message that sets the behaviour of the assistant before any user input is processed. LangGraph exposes this functionality via its `prompts` module. There you will find a few classes: `PromptTemplate` for formatting a single string, `ChatPromptTemplate` for building multi-message prompts, and `MessagesPlaceholder` for inserting a slot where conversation history will later be filled.\n",
    "\n",
    "In this exercise you will define a `ChatPromptTemplate` that begins with a system instruction. The system message should describe the assistantâ€™s role (for example, instructing it to behave like a drug discovery researcher) and will be prepended to the userâ€™s request before being passed to the agent.\n",
    "\n",
    "#### API References: [Prompt Template](https://python.langchain.com/docs/concepts/prompt_templates/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4244b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data.\"),\n",
    "    (\"user\", \"{request}\")\n",
    "])\n",
    "\n",
    "prompt_values = prompt_template.invoke({\"request\": \"What is the mechanism of action of remdesivir and what bioactivity data is available for this compound?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad830ca",
   "metadata": {},
   "source": [
    "`prompt_values` is a dictionary, which can be invoke directly to the agent. Try out by simple invoke the agent with the `prompt_values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa2a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = agent.invoke(prompt_values, config=config)\n",
    "for msg in msgs['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09254490",
   "metadata": {},
   "source": [
    "### Exercise 8.2 â€“ Integrate the system prompt with a streaming loop\n",
    "\n",
    "Building on the previous exercise, implement a multi-turn chat loop. Use a `while` loop to repeatedly accept user input, stream the agentâ€™s response, and append both user and assistant messages to the conversation state.\n",
    "\n",
    "To include your system instruction in every turn, apply the `ChatPromptTemplate` from exercise 8.1 when constructing the input for the agent. \n",
    "\n",
    "When using the streaming API you must supply a `state` dictionary that includes a `'messages'` key. The value of this key should be a list of message dictionaries (each with `role` and `content` fields) representing the conversation history. \n",
    "\n",
    "As you loop, update this list to maintain the context across turns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"example_conversation\"\n",
    "    },\n",
    "    \"recursion_limit\": 25\n",
    "}\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert drug discovery researcher. Use your available tools to answer the userâ€™s question as accurately as possible. Never fabricate or invent data.\"),\n",
    "    (\"human\", \"{request}\"),\n",
    "])\n",
    "\n",
    "print(\"Welcome to the LangGraph ReAct demo with memory! Ask me a question.\")\n",
    "print(\"I can perform simple math, look up drug information, and search PubMed via LitSense.\")\n",
    "print(\"====================================================================\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "    except EOFError:\n",
    "        break\n",
    "\n",
    "    if not user_input or user_input.lower() in {\"quit\", \"exit\", \"q\"}:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Build the state as a dict with a messages list\n",
    "    prompt_values = prompt_template.invoke({\"request\": user_input})\n",
    "    state = {\"messages\": prompt_values.messages}\n",
    "\n",
    "    # Print the user's message first\n",
    "    print(f\"\"\"================================== User's Message ==================================\n",
    "{user_input}\n",
    "    \"\"\")\n",
    "\n",
    "    # Stream agent events\n",
    "    for event in agent.stream(state, config=config):\n",
    "        for value in event.values():\n",
    "            msg = value[\"messages\"][-1]\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac0995",
   "metadata": {},
   "source": [
    "### Exercise 9.1 - Structure Output Parser\n",
    "\n",
    "Structured output parsers let you control the format of the language modelâ€™s responses. Instead of returning freeâ€‘form text, the model follows a schema (such as a Pydantic model) so that other components can reliably consume its output.\n",
    "\n",
    "- **Why?** Downstream software often expects data in a specific JSON schema (e.g. keyâ€“value pairs).\n",
    "- **How?** Define a Pydantic `BaseModel` to describe the fields you need, then use `with_structured_output(...)` to attach this schema to your LLM.\n",
    "\n",
    "You can read more in the [LangGraph structured output guide](https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/#define-model-tools-and-graph-state).\n",
    "\n",
    "The diagram below shows the highâ€‘level structure of the system weâ€™re about to build. One LLM is responsible for deciding when to call tools, and a second LLM is tasked with formatting the final response according to your schema.\n",
    "\n",
    "<img src=\"images/react_structured_output.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7131f6",
   "metadata": {},
   "source": [
    "First, we'll set up two language models:\n",
    "\n",
    "1. **Toolâ€‘calling LLM**: This model can invoke the tools you defined earlier (calculator, literature search, etc.).\n",
    "2. **Structured output LLM**: This model will use a Pydantic schema to return the final answer as JSON with fields for drug names, justifications, and sources.\n",
    "\n",
    "When we later ask a question such as *\"Use LitSearch literature, give me two antiviral drugs that are potentially to treat COVIDâ€‘19\"*, the agent will search PubMed via LitSense, find candidate antivirals, and then format its answer using this schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StepÂ 1: create the tool-calling LLM\n",
    "llm = ChatOpenAI(model='gpt-5', temperature=0)\n",
    "llm_with_tools = llm.bind_tools(TOOLS)\n",
    "\n",
    "# StepÂ 2: define a Pydantic schema describing the structured output we want\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DrugsInfo(BaseModel):\n",
    "    drugs: str = Field(description='a list of drug names')\n",
    "    justifications: str = Field(description='a list of justification for each drugs')\n",
    "    sources: str = Field(description='a list of citation for each drugs')\n",
    "\n",
    "# StepÂ 3: create the structured output LLM\n",
    "llm = ChatOpenAI(model='gpt-5', temperature=0)\n",
    "llm_with_structured_outputs = llm.with_structured_output(DrugsInfo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c144fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Dict, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Define the chatbot node: this function invokes the toolâ€‘calling LLM and appends the AI message to state\n",
    "def chatbot(state: ChatState) -> Dict[str, List]:\n",
    "    \"\"\"The main chatbot node. It invokes the LLM with the current messages.\"\"\"\n",
    "    return {'messages': [llm_with_tools.invoke(state['messages'])]}\n",
    "\n",
    "# Define the respond node: this function uses the structuredâ€‘output LLM to format the final answer\n",
    "def respond(state: ChatState):\n",
    "    # Extract only the content of each message for the structured LLM\n",
    "    response = llm_with_structured_outputs.invoke(\n",
    "        [HumanMessage(content=state[\"messages\"][i].content) for i in range(len(state[\"messages\"]))]\n",
    "    )\n",
    "    # Wrap the structured JSON as an AIMessage and return it\n",
    "    return {\"messages\": [AIMessage(content=response.model_dump_json())]}\n",
    "\n",
    "# Define a simple routing function to decide whether to keep calling tools or format a final response\n",
    "def route_tools(state: ChatState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the last message does not contain a tool call, we are ready to format a response\n",
    "    if not last_message.tool_calls:\n",
    "        return \"respond\"\n",
    "    return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d493a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the state graph\n",
    "workflow = StateGraph(ChatState)\n",
    "\n",
    "# Add the nodes: chatbot (for reasoning/tool calls), respond (for final formatting), and the existing tool node\n",
    "workflow.add_node(\"chatbot\", chatbot)\n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entry point: we start by invoking the chatbot node\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "\n",
    "# Add conditional edges: route to either tools or respond depending on should_continue\n",
    "workflow.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"respond\": \"respond\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# After calling tools, loop back to the chatbot; after responding, end the workflow\n",
    "workflow.add_edge(\"tools\", \"chatbot\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "\n",
    "# Compile the workflow into an executable agent\n",
    "agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b827f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    print('Graph visualization not available in this environment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0605e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the agent a question and prettyâ€‘print the structured response\n",
    "answer = agent.invoke({'messages': 'Use LitSearch literatures, give me 2 antiviral drugs that are potential to treat COVID-19'})\n",
    "answer['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd59a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Nodes** are functions that operate on state.\n",
    "- **Edges** define the flow between nodes.\n",
    "- **State** carries data through the agent. Use reducer functions such as `add_messages` to control how the state is updated.\n",
    "- **Tools** extend agent capabilities and are annotated with `@tool`.\n",
    "- **Memory** allows agents to maintain context across invocations.\n",
    "- **Prebuilt agents** provide quick solutions for common patterns but offer less control than a custom graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa566d4f",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "\n",
    "- **LangChain Documentation** â€“ https://docs.langchain.com/oss/python/langchain/overview\n",
    "- **LangGraph Documentation** â€“ https://docs.langchain.com/oss/python/langgraph/overview\n",
    "- **ChEMBL Web Services** â€“ https://chembl.gitbook.io/chembl-interface-documentation/web-services\n",
    "- **OpenAI API** â€“ https://platform.openai.com/docs/\n",
    "\n",
    "These references can help you explore LangGraph and related libraries beyond the scope of this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5952c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You've built your first AI agent with LangGraph!**\n",
    "\n",
    "Feel free to modify and extend your agent. You can experiment with new tools, different LLMs, and additional nodes to create even more capable and personalised agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scilifelab_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
